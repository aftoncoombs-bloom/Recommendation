{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../../../scripts/')\n",
    "from s3_support import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load & prep data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### google traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '''select\n",
    "            date_trunc('week', date) as date,\n",
    "            org,\n",
    "            form,\n",
    "            sum(views) as pageviews\n",
    "        from googleanalytics_traffic\n",
    "            where date>=2018\n",
    "        group by date_trunc('week', date), org, form;'''\n",
    "pageviews = redshift_query_read(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(677307,\n",
       " 14382,\n",
       " Timestamp('2018-01-01 00:00:00'),\n",
       " Timestamp('2021-04-19 00:00:00'))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageviews = pageviews[pageviews['form']!=0]\n",
    "pageviews['date'] = pd.to_datetime(pageviews['date'])\n",
    "len(pageviews), len(pageviews['form'].unique()), pageviews['date'].min(), pageviews['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '''select \n",
    "            form, \n",
    "            date_trunc('week', date) as date,\n",
    "            count(id) as count, \n",
    "            sum(amount) as vol\n",
    "        from transactions\n",
    "        where status='A' and date>=2018 and source='p2p'\n",
    "        group by form, date_trunc('week', date)\n",
    "    '''\n",
    "trans = redshift_query_read(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45274,\n",
       " 4965,\n",
       " Timestamp('2018-01-01 00:00:00'),\n",
       " Timestamp('2021-04-26 00:00:00'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans['date'] = pd.to_datetime(trans['date'])\n",
    "len(trans), len(trans['form'].unique()), trans['date'].min(), trans['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with analytics\n",
      "done with analyticsqgiv\n"
     ]
    }
   ],
   "source": [
    "q = \"select * from analytics_weekly where date>=2018\"\n",
    "df_base = redshift_query_read(q)\n",
    "print(\"done with analytics\")\n",
    "\n",
    "q = \"select * from analyticsp2p_weekly where date>=2018\"\n",
    "df_p2p = redshift_query_read(q)\n",
    "print(\"done with analyticsqgiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2p_forms = df_p2p['form'].tolist()\n",
    "df_base = df_base[df_base['form'].isin(p2p_forms)]\n",
    "\n",
    "df_analytics = df_base.merge(df_p2p, on=[\"org\", \"form\", \"date\"]).dropna()\n",
    "\n",
    "df_analytics = df_analytics.drop(['org', 'product'], axis=1).groupby(['date', 'form']).sum().reset_index()\n",
    "\n",
    "df_analytics['date'] = pd.to_datetime(df_analytics['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2018-01-01 00:00:00'), Timestamp('2021-04-05 00:00:00'))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analytics['date'].min(), df_analytics['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow dataset to forms in p2p analytics\n",
    "pageviews = pageviews[pageviews['form'].isin(p2p_forms)]\n",
    "trans = trans[trans['form'].isin(p2p_forms)]\n",
    "\n",
    "# merge traffic and transactions\n",
    "trans_n_views = trans.merge(pageviews, on=['form', 'date'])\n",
    "trans_n_views.columns = ['form', 'date', 'trans_count', 'trans_vol', 'org', 'pageviews']\n",
    "\n",
    "# add conversion & average trans value\n",
    "trans_n_views['conversion'] = trans_n_views['trans_count'] / trans_n_views['pageviews']\n",
    "trans_n_views['avg_trans'] = trans_n_views['trans_vol'] / trans_n_views['trans_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_an = df_analytics.merge(trans_n_views, on=['form', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1842, 229, Timestamp('2018-01-15 00:00:00'), Timestamp('2021-04-05 00:00:00'))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_an), len(df_an['form'].unique()), df_an['date'].min(), df_an['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering & modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "CLUSTERS = [2, 3, 4, 5, 9, 12, 15, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['avg_trans', 'date', 'form', 'trans_count', 'trans_vol', \n",
    "             'org', 'pageviews', 'reg_count', 'sub_reg_count', \n",
    "             'don_count']\n",
    "targets = 'conversion'\n",
    "ftrs = [c for c in df_an.columns if c not in targets and c not in drop_cols and '_trans_' not in c and '_volume' not in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling w/ 2 clusters\n",
      "----------------------------------------\n",
      "\tvariance explained: -8.206219247908267\n",
      "\tmse: 61.06092963163971\n",
      "\tmae: 0.7970638004916989\n",
      "\tr2: -8.306221562763204\n",
      "\n",
      "Modeling w/ 3 clusters\n",
      "----------------------------------------\n",
      "\tvariance explained: -6.746315316669204\n",
      "\tmse: 56.35581391492565\n",
      "\tmae: 0.761756636913821\n",
      "\tr2: -6.799354418413347\n",
      "\n",
      "Modeling w/ 4 clusters\n",
      "----------------------------------------\n",
      "\tvariance explained: -9.588833586158957\n",
      "\tmse: 61.29351267218627\n",
      "\tmae: 0.7619990856135372\n",
      "\tr2: -9.679860242209038\n",
      "\n",
      "Modeling w/ 5 clusters\n",
      "----------------------------------------\n",
      "\tvariance explained: -8.057701709724531\n",
      "\tmse: 58.77278780775065\n",
      "\tmae: 0.7658130338990848\n",
      "\tr2: -8.18068961581176\n",
      "\n",
      "Modeling w/ 9 clusters\n",
      "----------------------------------------\n",
      "\tvariance explained: -4.6500176925029395\n",
      "\tmse: 53.058638452257895\n",
      "\tmae: 0.7318839991900917\n",
      "\tr2: -4.691780869668669\n",
      "\n",
      "Modeling w/ 12 clusters\n",
      "----------------------------------------\n",
      "\tvariance explained: -5.540370855284468\n",
      "\tmse: 57.56992107418773\n",
      "\tmae: 0.7890920647826161\n",
      "\tr2: -5.6060191608957135\n",
      "\n",
      "Modeling w/ 15 clusters\n",
      "----------------------------------------\n",
      "\tvariance explained: -3.841285380900853\n",
      "\tmse: 63.78258627078451\n",
      "\tmae: 0.7784929758646261\n",
      "\tr2: -3.874803727636867\n",
      "\n",
      "Modeling w/ 24 clusters\n",
      "----------------------------------------\n",
      "\tvariance explained: -6.118211432587471\n",
      "\tmse: 51.061216995847666\n",
      "\tmae: 0.7286571143131174\n",
      "\tr2: -6.190669665914835\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cluster_count in CLUSTERS:\n",
    "    print(\"Modeling w/ {} clusters\".format(cluster_count))\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # set clusters\n",
    "    cluster_training_data = df_an.groupby('form')[ftrs].mean().reset_index()\n",
    "    kmeans = KMeans(n_clusters=cluster_count).fit(cluster_training_data)\n",
    "    cluster_training_data['meta_cluster'] = kmeans.labels_\n",
    "    df_an['meta_cluster'] = df_an['form'].apply(lambda x: cluster_training_data[cluster_training_data['form']==x]['meta_cluster'].iloc[0])\n",
    "    \n",
    "    # modelcluster\n",
    "\n",
    "    score_explained_variance = []\n",
    "    score_mse = []\n",
    "    score_mae = []\n",
    "    score_r2 = []\n",
    "    \n",
    "    for i in range(50):\n",
    "        # train/test data split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_an[ftrs + ['meta_cluster']], df_an['conversion'])\n",
    "        \n",
    "        rf = RandomForestRegressor()\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        \n",
    "        score_explained_variance.append(explained_variance_score(y_test, y_pred))\n",
    "        score_mse.append(mean_squared_error(y_test, y_pred))\n",
    "        score_mae.append(mean_absolute_error(y_test, y_pred))\n",
    "        score_r2.append(r2_score(y_test, y_pred))\n",
    "        \n",
    "    print(\"\\tvariance explained: {}\".format(np.mean(score_explained_variance)))\n",
    "    print(\"\\tmse: {}\".format(np.mean(score_mse)))\n",
    "    print(\"\\tmae: {}\".format(np.mean(score_mae)))\n",
    "    print(\"\\tr2: {}\".format(np.mean(score_r2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
